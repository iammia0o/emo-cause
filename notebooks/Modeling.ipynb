{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4d26c1-11ce-42f1-94f2-b2113af797ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.chunk import *\n",
    "from nltk.chunk.util import *\n",
    "from nltk.chunk.regexp import *\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e6c562-b303-40b3-b5ba-0672c4ff3cd2",
   "metadata": {},
   "source": [
    "### Data pre-processing plan \n",
    "- Clean tweets : strip out handles\n",
    "- Explore patterns\n",
    "- Filter out helpful pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71c51f00-c9af-431e-9a4f-5c7b94355ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_words = ['proud', 'emotional', 'tired', 'low', 'down', 'sick', 'nostalgic', 'stressed', 'uncomfortable', 'grateful', 'sorry', 'jealous','overwhelmed', 'unmotivated', 'exhausted', 'motivated', 'sentimental']\\\n",
    "           + ['drained', 'unmotivated', 'hungry', 'hopeless', 'insecure', 'adventurous', 'euphoric', 'accomplished', 'nauseous', 'shitty', 'awful', 'stupid', 'horrible', 'foolish']\n",
    "with open('emotions_updated.txt', 'r') as f:\n",
    "    emotion_ls = f.readlines()\n",
    "emotion_ls = [w.strip('\\n') for w in emotion_ls]\n",
    "emo_set = set(emotion_ls)\n",
    "# print(emo_set, len(emo_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4b3ef1-1026-4240-a085-7387c83b485a",
   "metadata": {},
   "source": [
    "## Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "554f51c3-f4e9-4efe-8ce5-a1d26e749ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79579458-9f5a-4e13-8127-64f4a63b4f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba9a537a-f7cb-40c1-9327-eb8bf6385d64",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09fe1bf6-ccc9-4bd3-92bb-da10894963c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_class=20, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers  = nn.Sequential(\n",
    "            nn.Linear(77, hidden_size * 4),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear( hidden_size * 4, hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size*2),\n",
    "            nn.Linear(hidden_size*2, hidden_size*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size*2, hidden_size*2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size*2),\n",
    "            nn.Linear(hidden_size*2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout), \n",
    "            nn.Linear(hidden_size, num_class),\n",
    "#             nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):        \n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5904ebd-d5e6-4729-8c2b-51ff46e6a641",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d5719-f42d-48ca-af09-df780b4f0f6f",
   "metadata": {},
   "source": [
    "I adapt the CNN architecture for text classification problem from [Zhang and Wallace 2015](https://arxiv.org/pdf/1510.03820.pdf) paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13dfe98b-04dc-4115-b76b-2afb71d77e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,num_embeddings,\n",
    "                         embed_dim=300,\n",
    "                         filter_sizes=[3, 4, 5],\n",
    "                         num_filters=[100, 100, 100],\n",
    "                         num_classes=20,\n",
    "                        dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                      embedding_dim=self.embed_dim,\n",
    "                                      padding_idx=0,\n",
    "                                      max_norm=5.0)\n",
    "    # Conv Network\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self.embed_dim,\n",
    "                      out_channels=num_filters[i],\n",
    "                      kernel_size=filter_sizes[i])\n",
    "            for i in range(len(filter_sizes))\n",
    "        ])\n",
    "        # Fully-connected layer and Dropout\n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_embed = self.embedding(x).float()\n",
    "\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\n",
    "\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "        \n",
    "\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "\n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3698beb-2890-48d5-afd7-c3926b971f9a",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78620c5-b75b-49ae-8e83-d096c7499e8c",
   "metadata": {},
   "source": [
    "To adapt RNN architecture for text classification problem, only final hidden state is used. This state is fed into a linear layer for class prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45da4678-b451-43e5-b926-bbd5d0580a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self,num_embeddings,\n",
    "                         embed_dim=64,\n",
    "                         hidden_size=32,\n",
    "                         num_classes=20,\n",
    "                        dropout=0.3):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer1 = nn.Embedding( num_embeddings  , embed_dim  )\n",
    "        self.layer2 = nn.GRU(       embed_dim , hidden_size  )\n",
    "        self.layer3 = nn.Linear(    hidden_size , num_classes   )\n",
    "\n",
    "        \n",
    "    def forward(self, word_seq ):\n",
    "        \n",
    "        h_init = torch.zeros(1, word_seq.shape[1], self.hidden_size).to(device)\n",
    "        \n",
    "        g_seq  =   self.layer1( word_seq )   \n",
    "        h_seq , h_final =   self.layer2( g_seq , h_init )\n",
    "        score_seq =   self.layer3(h_final.permute(1, 0, 2) )\n",
    "        \n",
    "        return score_seq.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3f315-c34b-4750-9073-dde8816b9f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c39a0aae-c40d-4394-885c-5318e28f8adc",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcb2e8-5240-4cdb-a1fe-bfe215654d59",
   "metadata": {},
   "source": [
    "This ANN model is addapted from Luong et al. 2016 model. To adapt to a text classification problem, attention machanism is applied to the final hidden state. After this attention layer, output is passed to a fully connected layer for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61747fa9-a9d3-4740-a5e1-0b50b1ebbeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self,num_embeddings,\n",
    "                         embed_dim=64,\n",
    "                         hidden_size=32,\n",
    "                         num_classes=20,\n",
    "                        dropout=0.3):\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed = nn.Embedding( num_embeddings  , embed_dim  )\n",
    "        self.encoder = nn.GRU(       embed_dim , hidden_size  )\n",
    "        self.Wc = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(    hidden_size  , num_classes   )\n",
    "\n",
    "    def forward(self, inp_seq):\n",
    "        \n",
    "        h_init = torch.zeros(1, inp_seq.shape[1], self.hidden_size).to(device)\n",
    "        g_seq               =   self.embed( inp_seq )   \n",
    "        h_seq , h_final     =   self.encoder( g_seq , h_init )\n",
    "\n",
    "        # attention machanism\n",
    "        s_init = h_final\n",
    "        s_seq_trans = torch.swapaxes(s_init, 0,1)\n",
    "        h_seq_trans = torch.swapaxes(torch.swapaxes(h_seq, 0, 2), 0, 1)\n",
    "   \n",
    "        bacthed_et = s_seq_trans.bmm(h_seq_trans)\n",
    "        batched_alpha_t = torch.softmax(bacthed_et, axis=2) # shape = batch_size * outseq_length * in_seq_length\n",
    "        #\n",
    "        batched_ct = torch.bmm(batched_alpha_t, torch.swapaxes(h_seq, 0, 1))\n",
    "        batched_ct = torch.swapaxes(batched_ct, 0, 1)\n",
    "        \n",
    "        batched_atten_t = torch.tanh(self.Wc(torch.cat((batched_ct, s_init), 2)) )\n",
    "#         batched_atten_t = torch.tanh(self.Wc(batched_ct) )\n",
    "    \n",
    "        out = self.out(batched_atten_t)\n",
    "#         out = torch.log_softmax(self.output(batched_atten_t), axis=2)\n",
    "    \n",
    "        return out.squeeze(axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6639bc5-9565-4fde-8dd8-b8c5ff918e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e3342f-8102-4483-b386-0131056379d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8000becb-2677-46b9-985f-797ae7940bcc",
   "metadata": {},
   "source": [
    "### Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61235d81-7eed-426b-a101-e21c3ff0b54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ffd018-af00-44b9-bbfe-edf79ef67701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc, time\n",
    "# For this dataset, we are trying to translate french to english\n",
    "SRC_LANGUAGE = 'fr'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "\n",
    "# First, we create a custom dataset to load the data. Each item is a pair of french and english datapoint\n",
    "class EmoCauseDataset(Dataset):\n",
    "    def __init__(self,path, train, train_size=10000, test_size=1000, max_len=250):\n",
    "        self.dir = path\n",
    "        self.all_data = None\n",
    "        self.data = None\n",
    "        self.train_size=train_size\n",
    "        self.test_size=test_size\n",
    "        with open(path, 'rb') as f:\n",
    "            self.all_data = pickle.load(f)\n",
    "        self.labels = list(zip(*self.all_data))[1]\n",
    "        self.label_map = {emo:i for i, emo in enumerate(list(set(self.labels)))}\n",
    "        if train:\n",
    "            self.data = self.all_data[:self.train_size]\n",
    "        else:\n",
    "            self.data = self.all_data[self.train_size + 1:self.train_size + self.test_size]\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx][0], self.label_map[self.data[idx][1]]\n",
    "    \n",
    "    def get_class_map(self):\n",
    "        return {v:k for k, v in self.label_map.items()}\n",
    "    def get_label(self):\n",
    "        return list(zip(*self.data))[1]\n",
    "    \n",
    "# Helper function to call token_transform\n",
    "def yield_tokens(data_iter: Iterable) -> List[str]:\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform(data_sample[0])\n",
    "\n",
    "\n",
    "# Functions transform the input sentence to a format that can be used for training \n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "def collate_fn(tweet, emo,batch_first=True):\n",
    "    max_len = 75\n",
    "    dummie = \" \".join(['dm'] * max_len)\n",
    "    tweet_batch, emo_batch = [], []\n",
    "    for tw, e in zip(tweet, emo):\n",
    "        tweet_batch.append(text_transform(tw.strip('\\n')))\n",
    "#         emo_batch.append(F.one_hot(torch.tensor(e), num_classes=20))\n",
    "        emo_batch.append(torch.tensor(e))\n",
    "    tweet_batch.append(text_transform(dummie))\n",
    "    tweet_batch = pad_sequence(tweet_batch, padding_value=PAD_IDX,batch_first=batch_first)\n",
    "    emo_batch = torch.stack(emo_batch)\n",
    "    if batch_first == True:\n",
    "        tweet_batch = tweet_batch[:-1]\n",
    "    else:\n",
    "        tweet_batch = tweet_batch[:, :-1]\n",
    "    return tweet_batch, emo_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3f10592-38f6-4366-b1a4-85b90eb33363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "token_transform = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "datapath = \"label_final.pickle\"\n",
    "dataset = EmoCauseDataset(datapath, train=True)\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "train_iter = iter(dataset)\n",
    "vocab_transform = build_vocab_from_iterator(yield_tokens(train_iter),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "vocab_transform.set_default_index(UNK_IDX)\n",
    "text_transform = sequential_transforms(token_transform, #Tokenization\n",
    "                                               vocab_transform, #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor        \n",
    "label_transform = sequential_transforms(token_transform)\n",
    "torch.manual_seed(0)\n",
    "VOCAB_SIZE = len(vocab_transform)\n",
    "# TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a886ea4-8500-4599-9152-a0472a807c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(model, x, y, x_type=torch.LongTensor):\n",
    "    optimizer.zero_grad()\n",
    "    # Transform inputs\n",
    "\n",
    "    # send them to the gpu\n",
    "    minibatch_data=x.type(x_type).to(device)\n",
    "    minibatch_label=y.type(torch.LongTensor).to(device)\n",
    "#     print(x.shape, y.shape)\n",
    "    \n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model(minibatch_data)\n",
    "#     print(y_pred.shape, minibatch_label.shape)\n",
    "#     print(y_pred)\n",
    "    loss = criterion(y_pred, minibatch_label)\n",
    "\n",
    "    # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "    loss.backward()\n",
    "\n",
    "    # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "    normalize_gradient(model)\n",
    "    optimizer.step()\n",
    "    # update the running loss  \n",
    "    return loss.detach().item()\n",
    "\n",
    "def normalize_gradient(net):\n",
    "\n",
    "    grad_norm_sq=0\n",
    "\n",
    "    for p in net.parameters():\n",
    "        grad_norm_sq += p.grad.data.norm()**2\n",
    "\n",
    "    grad_norm=math.sqrt(grad_norm_sq)\n",
    "\n",
    "    if grad_norm<1e-4:\n",
    "        net.zero_grad()\n",
    "        print('grad norm close to zero')\n",
    "    else:    \n",
    "        for p in net.parameters():\n",
    "            p.grad.data.div_(grad_norm)\n",
    "\n",
    "    return grad_norm\n",
    "\n",
    "def test(model, test_loader, class_map, text_type=torch.LongTensor, batch_first=True):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print('device', device)\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for text, label in test_loader:\n",
    "            text, _ = collate_fn(text, label, batch_first=batch_first)\n",
    "            texts = text.type(text_type).to(device)\n",
    "            \n",
    "            labels.append(label)\n",
    "            outputs = model(texts).cpu()\n",
    "            # get the label predictions\n",
    "            preds += outputs.argmax(dim=1).tolist()\n",
    "    \n",
    "#     preds = list(map(lambda x: class_map[x], preds))\n",
    "    return torch.tensor(preds), torch.cat(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817de335-ec9f-4161-9cce-ba4cf8dc27c4",
   "metadata": {},
   "source": [
    "### Test Model for MLP, CNN and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c28bd03-ed25-437d-8449-618d937e8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, my_lr, num_epochs,x_type=torch.LongTensor, batch_first=True):\n",
    "    for epoch in range(num_epochs):\n",
    "          # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
    "        if epoch % 4 ==0:\n",
    "            my_lr = my_lr / 1.05\n",
    "        start = time.time()\n",
    "\n",
    "        # set the running quantities to zero at the beginning of the epoch\n",
    "        running_loss=0\n",
    "        num_batches=0    \n",
    "        model.train()\n",
    "\n",
    "        for x, y in train_dataloader:\n",
    "\n",
    "            x, y = collate_fn(x, y, batch_first=batch_first)\n",
    "            loss = batch_train(model, x, y,x_type)      \n",
    "            # Set the gradients to zeros\n",
    "            running_loss += loss\n",
    "            num_batches += 1\n",
    "    #         # Collect garbage to prevent OOM\n",
    "            gc.collect()\n",
    "        # compute stats for the full training set\n",
    "        total_loss = running_loss / num_batches\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        print('')\n",
    "        print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd2d37-fd8f-453a-8f6e-cc4ab7720644",
   "metadata": {},
   "source": [
    "### MLP performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1137132-61db-4d04-b271-9fa69da0cabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-51a53ba6f319>:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emo_batch.append(torch.tensor(e))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 45.11897873878479 \t lr= 0.2857142857142857 \t exp(loss)= 22.124884881280536\n",
      "\n",
      "epoch= 1 \t time= 31.799460649490356 \t lr= 0.2857142857142857 \t exp(loss)= 22.59522294253321\n",
      "\n",
      "epoch= 2 \t time= 31.815052032470703 \t lr= 0.2857142857142857 \t exp(loss)= 22.651421265311726\n",
      "\n",
      "epoch= 3 \t time= 31.767410039901733 \t lr= 0.2857142857142857 \t exp(loss)= 22.2026917152874\n",
      "\n",
      "epoch= 4 \t time= 31.74264621734619 \t lr= 0.27210884353741494 \t exp(loss)= 22.501127122520277\n",
      "\n",
      "epoch= 5 \t time= 31.819934368133545 \t lr= 0.27210884353741494 \t exp(loss)= 22.356925556898158\n",
      "\n",
      "epoch= 6 \t time= 31.780929565429688 \t lr= 0.27210884353741494 \t exp(loss)= 22.511206437801604\n",
      "\n",
      "epoch= 7 \t time= 32.12287926673889 \t lr= 0.27210884353741494 \t exp(loss)= 22.677455107372147\n",
      "\n",
      "epoch= 8 \t time= 31.872369050979614 \t lr= 0.2591512795594428 \t exp(loss)= 22.627863395974444\n",
      "\n",
      "epoch= 9 \t time= 31.781724214553833 \t lr= 0.2591512795594428 \t exp(loss)= 22.772442456026084\n",
      "\n",
      "epoch= 10 \t time= 31.758777856826782 \t lr= 0.2591512795594428 \t exp(loss)= 22.593674000424944\n",
      "\n",
      "epoch= 11 \t time= 31.742796182632446 \t lr= 0.2591512795594428 \t exp(loss)= 22.49709378807706\n",
      "\n",
      "epoch= 12 \t time= 31.76646065711975 \t lr= 0.24681074243756454 \t exp(loss)= 22.50288483737535\n",
      "\n",
      "epoch= 13 \t time= 31.756405353546143 \t lr= 0.24681074243756454 \t exp(loss)= 22.51932623398526\n",
      "\n",
      "epoch= 14 \t time= 31.71126127243042 \t lr= 0.24681074243756454 \t exp(loss)= 22.410883488547128\n",
      "\n",
      "epoch= 15 \t time= 31.775909662246704 \t lr= 0.24681074243756454 \t exp(loss)= 22.687448905078185\n",
      "\n",
      "epoch= 16 \t time= 31.7605721950531 \t lr= 0.23505784994053763 \t exp(loss)= 22.797859054179227\n",
      "\n",
      "epoch= 17 \t time= 31.731371879577637 \t lr= 0.23505784994053763 \t exp(loss)= 22.77609244961226\n",
      "\n",
      "epoch= 18 \t time= 31.766388416290283 \t lr= 0.23505784994053763 \t exp(loss)= 22.655475106249533\n",
      "\n",
      "epoch= 19 \t time= 31.770638704299927 \t lr= 0.23505784994053763 \t exp(loss)= 22.371303545354568\n",
      "\n",
      "epoch= 20 \t time= 31.742231607437134 \t lr= 0.22386461899098822 \t exp(loss)= 22.55171309608213\n",
      "\n",
      "epoch= 21 \t time= 31.875935792922974 \t lr= 0.22386461899098822 \t exp(loss)= 22.472813201054638\n",
      "\n",
      "epoch= 22 \t time= 32.18969702720642 \t lr= 0.22386461899098822 \t exp(loss)= 22.4041235533108\n",
      "\n",
      "epoch= 23 \t time= 32.1893527507782 \t lr= 0.22386461899098822 \t exp(loss)= 22.54641195428199\n",
      "\n",
      "epoch= 24 \t time= 32.161247968673706 \t lr= 0.2132043990390364 \t exp(loss)= 22.41063050021731\n",
      "\n",
      "epoch= 25 \t time= 32.18036484718323 \t lr= 0.2132043990390364 \t exp(loss)= 22.448357268223617\n",
      "\n",
      "epoch= 26 \t time= 32.20997738838196 \t lr= 0.2132043990390364 \t exp(loss)= 22.729571123836173\n",
      "\n",
      "epoch= 27 \t time= 32.17955827713013 \t lr= 0.2132043990390364 \t exp(loss)= 22.481629556683654\n",
      "\n",
      "epoch= 28 \t time= 32.15771245956421 \t lr= 0.20305180860860608 \t exp(loss)= 22.826044822938222\n",
      "\n",
      "epoch= 29 \t time= 32.208081007003784 \t lr= 0.20305180860860608 \t exp(loss)= 22.59086253194222\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-09d826099cf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_acc' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 30\n",
    "hidden_size = 32\n",
    "my_lr = 0.3\n",
    "bs = 32\n",
    "num_class=20\n",
    "\n",
    "# Best setting params:\n",
    "#     CNN: hidden_size=64, num_epochs=10, my_lr=0.3, bs=32\n",
    "#     RNN: hidden_size=32, num_epochs=40, my_lr=0.5, bs=32\n",
    "# The code below is taken fromthe VRNN demo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Variables\n",
    "VOCAB_SIZE = len(vocab_transform)\n",
    "train_dataset = EmoCauseDataset(datapath,train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "test_dataset = EmoCauseDataset(datapath, train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "\n",
    "model = MLP(VOCAB_SIZE,\n",
    "         hidden_size=hidden_size, \n",
    "        num_class=20, dropout=0.3).to(device)\n",
    "\n",
    "batch_first=True # batch_first is all in RNN, set = true in other nets \n",
    "\n",
    "# model.layers.weight.data.uniform_(-0.1, 0.1)\n",
    "# model.layer3.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "\n",
    "# model = LSTMClassifier(batch_size=bs, output_size=num_class, hidden_size=hidden_size, vocab_size=VOCAB_SIZE, embedding_length=300).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=my_lr, momentum=0.9)\n",
    "\n",
    "train(model, my_lr, num_epochs,torch.FloatTensor,batch_first)\n",
    "gc.collect()\n",
    "print(\"Accuracy = \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46e58397-5cbf-4a14-9bf1-7fc153c6601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-51a53ba6f319>:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emo_batch.append(torch.tensor(e))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  tensor(0.0470)\n"
     ]
    }
   ],
   "source": [
    "y_preds, y_test = test(model,test_dataloader, class_map=test_dataset.get_class_map(),text_type=torch.FloatTensor, batch_first=batch_first)\n",
    "# torch.tensor(y_preds).shape\n",
    "test_acc = torch.sum(y_preds == y_test) / 1000\n",
    "print(\"Accuracy = \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c52d813-4ccf-46c5-b604-dfa3ccb7e587",
   "metadata": {},
   "source": [
    "### CNN performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d01b5a6-88e2-4fbc-be46-230cd08e7315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-51a53ba6f319>:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emo_batch.append(torch.tensor(e))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 18.29328465461731 \t lr= 0.2857142857142857 \t exp(loss)= 15.25701819413573\n",
      "\n",
      "epoch= 1 \t time= 17.222455501556396 \t lr= 0.2857142857142857 \t exp(loss)= 10.194853318502256\n",
      "\n",
      "epoch= 2 \t time= 17.1863374710083 \t lr= 0.2857142857142857 \t exp(loss)= 8.578942864613973\n",
      "\n",
      "epoch= 3 \t time= 17.202964544296265 \t lr= 0.2857142857142857 \t exp(loss)= 7.0039026438655245\n",
      "\n",
      "epoch= 4 \t time= 17.174224853515625 \t lr= 0.27210884353741494 \t exp(loss)= 5.785516845638894\n",
      "\n",
      "epoch= 5 \t time= 17.134090185165405 \t lr= 0.27210884353741494 \t exp(loss)= 4.930101995499087\n",
      "\n",
      "epoch= 6 \t time= 17.202319383621216 \t lr= 0.27210884353741494 \t exp(loss)= 4.374101009213957\n",
      "\n",
      "epoch= 7 \t time= 17.134012699127197 \t lr= 0.27210884353741494 \t exp(loss)= 3.8457467850631324\n",
      "\n",
      "epoch= 8 \t time= 17.175636291503906 \t lr= 0.2591512795594428 \t exp(loss)= 3.5468072246810864\n",
      "\n",
      "epoch= 9 \t time= 17.178335905075073 \t lr= 0.2591512795594428 \t exp(loss)= 3.4099213907276136\n",
      "\n",
      "epoch= 10 \t time= 17.142795085906982 \t lr= 0.2591512795594428 \t exp(loss)= 3.152353614120474\n",
      "\n",
      "epoch= 11 \t time= 17.216025590896606 \t lr= 0.2591512795594428 \t exp(loss)= 3.033750679613839\n",
      "\n",
      "epoch= 12 \t time= 17.154917001724243 \t lr= 0.24681074243756454 \t exp(loss)= 2.9371397258500966\n",
      "\n",
      "epoch= 13 \t time= 17.160497903823853 \t lr= 0.24681074243756454 \t exp(loss)= 2.694477148329753\n",
      "\n",
      "epoch= 14 \t time= 17.19380497932434 \t lr= 0.24681074243756454 \t exp(loss)= 2.6834809143574976\n",
      "\n",
      "epoch= 15 \t time= 17.15047001838684 \t lr= 0.24681074243756454 \t exp(loss)= 2.5434584888073544\n",
      "\n",
      "epoch= 16 \t time= 17.196287631988525 \t lr= 0.23505784994053763 \t exp(loss)= 2.4843354671380395\n",
      "\n",
      "epoch= 17 \t time= 17.170743703842163 \t lr= 0.23505784994053763 \t exp(loss)= 2.5685754082446146\n",
      "\n",
      "epoch= 18 \t time= 17.1605486869812 \t lr= 0.23505784994053763 \t exp(loss)= 2.479582680331543\n",
      "\n",
      "epoch= 19 \t time= 17.18779730796814 \t lr= 0.23505784994053763 \t exp(loss)= 2.426605727102954\n",
      "\n",
      "epoch= 20 \t time= 17.141876697540283 \t lr= 0.22386461899098822 \t exp(loss)= 2.3764568877223766\n",
      "\n",
      "epoch= 21 \t time= 17.196295261383057 \t lr= 0.22386461899098822 \t exp(loss)= 2.378650140393388\n",
      "\n",
      "epoch= 22 \t time= 17.167067527770996 \t lr= 0.22386461899098822 \t exp(loss)= 2.3420852520063153\n",
      "\n",
      "epoch= 23 \t time= 17.135620832443237 \t lr= 0.22386461899098822 \t exp(loss)= 2.3131870307497366\n",
      "\n",
      "epoch= 24 \t time= 17.230273246765137 \t lr= 0.2132043990390364 \t exp(loss)= 2.4463871254421155\n",
      "\n",
      "epoch= 25 \t time= 17.145212411880493 \t lr= 0.2132043990390364 \t exp(loss)= 2.357355966569311\n",
      "\n",
      "epoch= 26 \t time= 17.017082691192627 \t lr= 0.2132043990390364 \t exp(loss)= 2.299514220766201\n",
      "\n",
      "epoch= 27 \t time= 16.96896266937256 \t lr= 0.2132043990390364 \t exp(loss)= 2.2458998610055723\n",
      "\n",
      "epoch= 28 \t time= 16.96213459968567 \t lr= 0.20305180860860608 \t exp(loss)= 2.2888735106768707\n",
      "\n",
      "epoch= 29 \t time= 17.035654544830322 \t lr= 0.20305180860860608 \t exp(loss)= 2.1783094619709997\n",
      "device cuda\n",
      "Accuracy =  tensor(0.2920)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 30\n",
    "hidden_size = 32\n",
    "my_lr = 0.3\n",
    "bs = 64\n",
    "num_class=20\n",
    "\n",
    "# Best setting params:\n",
    "#     CNN: hidden_size=64, num_epochs=10, my_lr=0.3, bs=32\n",
    "#     RNN: hidden_size=32, num_epochs=40, my_lr=0.5, bs=32\n",
    "# The code below is taken fromthe VRNN demo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Variables\n",
    "VOCAB_SIZE = len(vocab_transform)\n",
    "train_dataset = EmoCauseDataset(datapath,train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "test_dataset = EmoCauseDataset(datapath, train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "\n",
    "model = CNN(VOCAB_SIZE,\n",
    "             embed_dim=64,\n",
    "             filter_sizes=[3, 4, 5],\n",
    "            num_filters=[100, 100, 100],\n",
    "             num_classes=20,\n",
    "            dropout=0.3).to(device)\n",
    "\n",
    "batch_first=True # batch_first is all in RNN, set = true in other nets \n",
    "\n",
    "# model.layers.weight.data.uniform_(-0.1, 0.1)\n",
    "# model.layer3.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=my_lr, momentum=0.9)\n",
    "\n",
    "train(model, my_lr, num_epochs,torch.LongTensor, batch_first)\n",
    "gc.collect()\n",
    "\n",
    "y_preds, y_test = test(model,test_dataloader, class_map=test_dataset.get_class_map(),text_type=torch.LongTensor, batch_first=batch_first)\n",
    "# torch.tensor(y_preds).shape\n",
    "test_acc = torch.sum(y_preds == y_test) / 1000\n",
    "print(\"Accuracy = \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1830bc5-3398-48d9-be14-b986291cafc0",
   "metadata": {},
   "source": [
    "### RNN performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afb2b896-e8ff-4db3-96ac-c3d1f684aabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-51a53ba6f319>:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emo_batch.append(torch.tensor(e))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 32.81334686279297 \t lr= 0.2857142857142857 \t exp(loss)= 22.627375464142165\n",
      "\n",
      "epoch= 1 \t time= 32.52312612533569 \t lr= 0.2857142857142857 \t exp(loss)= 27.387163778826494\n",
      "\n",
      "epoch= 2 \t time= 32.568135499954224 \t lr= 0.2857142857142857 \t exp(loss)= 16.969647195479673\n",
      "\n",
      "epoch= 3 \t time= 32.5645067691803 \t lr= 0.2857142857142857 \t exp(loss)= 11.672814302990439\n",
      "\n",
      "epoch= 4 \t time= 32.56657338142395 \t lr= 0.27210884353741494 \t exp(loss)= 11.102341513752233\n",
      "\n",
      "epoch= 5 \t time= 32.54464817047119 \t lr= 0.27210884353741494 \t exp(loss)= 10.667305271497806\n",
      "\n",
      "epoch= 6 \t time= 32.581042766571045 \t lr= 0.27210884353741494 \t exp(loss)= 10.77504275683238\n",
      "\n",
      "epoch= 7 \t time= 32.58101987838745 \t lr= 0.27210884353741494 \t exp(loss)= 10.654790155778093\n",
      "\n",
      "epoch= 8 \t time= 32.521653175354004 \t lr= 0.2591512795594428 \t exp(loss)= 10.593118823978825\n",
      "\n",
      "epoch= 9 \t time= 32.54378914833069 \t lr= 0.2591512795594428 \t exp(loss)= 10.338412702581197\n",
      "\n",
      "epoch= 10 \t time= 32.81305432319641 \t lr= 0.2591512795594428 \t exp(loss)= 10.669990671132604\n",
      "\n",
      "epoch= 11 \t time= 32.723241329193115 \t lr= 0.2591512795594428 \t exp(loss)= 10.27699152832834\n",
      "\n",
      "epoch= 12 \t time= 32.57443714141846 \t lr= 0.24681074243756454 \t exp(loss)= 10.409807297580642\n",
      "\n",
      "epoch= 13 \t time= 32.59093403816223 \t lr= 0.24681074243756454 \t exp(loss)= 10.078157549532115\n",
      "\n",
      "epoch= 14 \t time= 32.60880494117737 \t lr= 0.24681074243756454 \t exp(loss)= 10.372769040494015\n",
      "\n",
      "epoch= 15 \t time= 32.619791984558105 \t lr= 0.24681074243756454 \t exp(loss)= 9.59226609966174\n",
      "\n",
      "epoch= 16 \t time= 32.58057713508606 \t lr= 0.23505784994053763 \t exp(loss)= 9.035307830210062\n",
      "\n",
      "epoch= 17 \t time= 32.5947744846344 \t lr= 0.23505784994053763 \t exp(loss)= 9.007573609784012\n",
      "\n",
      "epoch= 18 \t time= 32.62845993041992 \t lr= 0.23505784994053763 \t exp(loss)= 8.926792289973884\n",
      "\n",
      "epoch= 19 \t time= 32.608983278274536 \t lr= 0.23505784994053763 \t exp(loss)= 9.058968575379392\n",
      "\n",
      "epoch= 20 \t time= 32.57405471801758 \t lr= 0.22386461899098822 \t exp(loss)= 9.194898595559339\n",
      "\n",
      "epoch= 21 \t time= 32.58182668685913 \t lr= 0.22386461899098822 \t exp(loss)= 8.863424178827982\n",
      "\n",
      "epoch= 22 \t time= 32.62159323692322 \t lr= 0.22386461899098822 \t exp(loss)= 8.38871146800701\n",
      "\n",
      "epoch= 23 \t time= 32.62274956703186 \t lr= 0.22386461899098822 \t exp(loss)= 8.267857246962663\n",
      "\n",
      "epoch= 24 \t time= 32.65058445930481 \t lr= 0.2132043990390364 \t exp(loss)= 8.087813789448909\n",
      "\n",
      "epoch= 25 \t time= 32.65821361541748 \t lr= 0.2132043990390364 \t exp(loss)= 8.033501880897745\n",
      "\n",
      "epoch= 26 \t time= 32.67779040336609 \t lr= 0.2132043990390364 \t exp(loss)= 7.573489192209762\n",
      "\n",
      "epoch= 27 \t time= 32.68132019042969 \t lr= 0.2132043990390364 \t exp(loss)= 7.477987060805453\n",
      "\n",
      "epoch= 28 \t time= 32.600712299346924 \t lr= 0.20305180860860608 \t exp(loss)= 7.402537722203134\n",
      "\n",
      "epoch= 29 \t time= 32.593042850494385 \t lr= 0.20305180860860608 \t exp(loss)= 7.327947651012088\n",
      "device cuda\n",
      "Accuracy =  tensor(0.2800)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 30\n",
    "hidden_size = 32\n",
    "my_lr = 0.3\n",
    "bs = 32\n",
    "num_class=20\n",
    "\n",
    "# Best setting params:\n",
    "#     CNN: hidden_size=64, num_epochs=10, my_lr=0.3, bs=32\n",
    "#     RNN: hidden_size=32, num_epochs=40, my_lr=0.5, bs=32\n",
    "# The code below is taken fromthe VRNN demo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Variables\n",
    "VOCAB_SIZE = len(vocab_transform)\n",
    "train_dataset = EmoCauseDataset(datapath,train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "test_dataset = EmoCauseDataset(datapath, train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "\n",
    "model = RNN(VOCAB_SIZE,\n",
    "             embed_dim=64,\n",
    "             hidden_size=20,\n",
    "             num_classes=20,\n",
    "            dropout=0.3).to(device)\n",
    "\n",
    "batch_first=False # batch_first is all in RNN, set = true in other nets \n",
    "\n",
    "model.layer1.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "model.layer3.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "\n",
    "# model = LSTMClassifier(batch_size=bs, output_size=num_class, hidden_size=hidden_size, vocab_size=VOCAB_SIZE, embedding_length=300).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=my_lr, momentum=0.9)\n",
    "\n",
    "train(model, my_lr, num_epochs,torch.LongTensor, batch_first)\n",
    "gc.collect()\n",
    "y_preds, y_test = test(model,test_dataloader, class_map=test_dataset.get_class_map(),text_type=torch.LongTensor, batch_first=batch_first)\n",
    "# torch.tensor(y_preds).shape\n",
    "test_acc = torch.sum(y_preds == y_test) / 1000\n",
    "print(\"Accuracy = \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8ef8b-57dc-4fd3-812a-9791315e3b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30823162-0c2f-4b6e-8be9-8e8316340624",
   "metadata": {},
   "source": [
    "### ANN performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d903f178-531b-4994-9ddc-6019e74891a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-51a53ba6f319>:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emo_batch.append(torch.tensor(e))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 5.426100730895996 \t lr= 0.2857142857142857 \t exp(loss)= 20.53835859861445\n",
      "\n",
      "epoch= 1 \t time= 5.398729562759399 \t lr= 0.2857142857142857 \t exp(loss)= 20.671087840278794\n",
      "\n",
      "epoch= 2 \t time= 5.266108751296997 \t lr= 0.2857142857142857 \t exp(loss)= 20.548292295526657\n",
      "\n",
      "epoch= 3 \t time= 5.272995233535767 \t lr= 0.2857142857142857 \t exp(loss)= 17.621593129993542\n",
      "\n",
      "epoch= 4 \t time= 5.223921060562134 \t lr= 0.27210884353741494 \t exp(loss)= 13.289746529032968\n",
      "\n",
      "epoch= 5 \t time= 5.27517294883728 \t lr= 0.27210884353741494 \t exp(loss)= 11.021746836301215\n",
      "\n",
      "epoch= 6 \t time= 5.228762865066528 \t lr= 0.27210884353741494 \t exp(loss)= 10.598081418778436\n",
      "\n",
      "epoch= 7 \t time= 5.274895191192627 \t lr= 0.27210884353741494 \t exp(loss)= 10.71849047329211\n",
      "\n",
      "epoch= 8 \t time= 5.242192029953003 \t lr= 0.2591512795594428 \t exp(loss)= 10.200056955116363\n",
      "\n",
      "epoch= 9 \t time= 5.295134782791138 \t lr= 0.2591512795594428 \t exp(loss)= 10.015129637201584\n",
      "\n",
      "epoch= 10 \t time= 5.226464509963989 \t lr= 0.2591512795594428 \t exp(loss)= 9.774254793985303\n",
      "\n",
      "epoch= 11 \t time= 5.302780389785767 \t lr= 0.2591512795594428 \t exp(loss)= 10.03247368733341\n",
      "\n",
      "epoch= 12 \t time= 5.468976736068726 \t lr= 0.24681074243756454 \t exp(loss)= 9.176962288493153\n",
      "\n",
      "epoch= 13 \t time= 5.293119430541992 \t lr= 0.24681074243756454 \t exp(loss)= 8.465296907329\n",
      "\n",
      "epoch= 14 \t time= 5.243590354919434 \t lr= 0.24681074243756454 \t exp(loss)= 8.071105621483742\n",
      "\n",
      "epoch= 15 \t time= 5.362642526626587 \t lr= 0.24681074243756454 \t exp(loss)= 7.7740902628359585\n",
      "\n",
      "epoch= 16 \t time= 5.8282341957092285 \t lr= 0.23505784994053763 \t exp(loss)= 7.547211603759241\n",
      "\n",
      "epoch= 17 \t time= 5.368744373321533 \t lr= 0.23505784994053763 \t exp(loss)= 7.471727777459402\n",
      "\n",
      "epoch= 18 \t time= 5.352478265762329 \t lr= 0.23505784994053763 \t exp(loss)= 6.977512178393842\n",
      "\n",
      "epoch= 19 \t time= 5.34515905380249 \t lr= 0.23505784994053763 \t exp(loss)= 7.283973111407945\n",
      "\n",
      "epoch= 20 \t time= 5.339818716049194 \t lr= 0.22386461899098822 \t exp(loss)= 6.916383490104135\n",
      "\n",
      "epoch= 21 \t time= 5.794197082519531 \t lr= 0.22386461899098822 \t exp(loss)= 6.553436863531073\n",
      "\n",
      "epoch= 22 \t time= 5.483658075332642 \t lr= 0.22386461899098822 \t exp(loss)= 6.464081933284327\n",
      "\n",
      "epoch= 23 \t time= 5.470890283584595 \t lr= 0.22386461899098822 \t exp(loss)= 6.3761267921531175\n",
      "\n",
      "epoch= 24 \t time= 5.345288276672363 \t lr= 0.2132043990390364 \t exp(loss)= 5.732524194856086\n",
      "\n",
      "epoch= 25 \t time= 5.575717210769653 \t lr= 0.2132043990390364 \t exp(loss)= 5.309459633665206\n",
      "\n",
      "epoch= 26 \t time= 5.363619089126587 \t lr= 0.2132043990390364 \t exp(loss)= 4.980612769940614\n",
      "\n",
      "epoch= 27 \t time= 5.334388971328735 \t lr= 0.2132043990390364 \t exp(loss)= 4.627355130356753\n",
      "\n",
      "epoch= 28 \t time= 5.386007070541382 \t lr= 0.20305180860860608 \t exp(loss)= 4.426157947279893\n",
      "\n",
      "epoch= 29 \t time= 5.354672193527222 \t lr= 0.20305180860860608 \t exp(loss)= 4.082578111543266\n",
      "device cuda\n",
      "Accuracy =  tensor(0.3210)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# Hyperparameters\n",
    "num_epochs = 30\n",
    "hidden_size = 20\n",
    "my_lr = 0.3\n",
    "bs = 256\n",
    "num_class=20\n",
    "batch_first=False\n",
    "datapath = \"label_final.pickle\"\n",
    "\n",
    "# The code below is taken fromthe VRNN demo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Variables\n",
    "VOCAB_SIZE = len(vocab_transform)\n",
    "train_dataset = EmoCauseDataset(datapath,train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "test_dataset = EmoCauseDataset(datapath, train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "\n",
    "model = ANN(VOCAB_SIZE,\n",
    "             embed_dim=64,\n",
    "             hidden_size=20,\n",
    "             num_classes=20,\n",
    "            dropout=0.3).to(device)\n",
    "model.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "model.out.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "# model = LSTMClassifier(batch_size=bs, output_size=num_class, hidden_size=hidden_size, vocab_size=VOCAB_SIZE, embedding_length=300).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=my_lr, momentum=0.9)\n",
    "gc.collect()\n",
    "train(model, my_lr, num_epochs,torch.LongTensor, batch_first)\n",
    "y_preds, y_test = test(model,test_dataloader, class_map=test_dataset.get_class_map(),text_type=torch.LongTensor, batch_first=batch_first)\n",
    "# torch.tensor(y_preds).shape\n",
    "test_acc = torch.sum(y_preds == y_test) / 1000\n",
    "print(\"Accuracy = \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef16b0-d9cf-4c41-8ac9-60cdedcaa153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76157bdb-c872-4541-987e-4e3a9292be3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ed827-b9d1-4bdc-b939-d92a89439c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8d80d-fb1e-44f8-a8c8-288ce753ddf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-1.9",
   "language": "python",
   "name": "torch-1.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
